{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=  0 x =  -4.0 y =  4.0 f= 14464.0\n",
      "iter=  1 x =  2.182397788748313 y =  4.758858698800743 f= 3.3052789261719275\n",
      "iter=  2 x =  2.20193967876644 y =  4.865560811132463 f= 3.261997340751402\n",
      "iter=  3 x =  2.420764626888233 y =  5.825488906673381 f= 2.6137866863128587\n",
      "iter=  4 x =  2.6311213679182024 y =  6.873709815906528 f= 2.114809917165767\n",
      "iter=  5 x =  2.8310822043435646 y =  7.961585777076442 f= 1.6519593412149989\n",
      "iter=  6 x =  3.0434164556181056 y =  9.213143432646714 f= 1.1575126901627943\n",
      "iter=  7 x =  3.323266465920134 y =  11.012816737078122 f= 0.5558325520086083\n",
      "iter=  8 x =  4.030527327371058 y =  16.246149758939502 f= 0.0010317622278332651\n",
      "iter=  9 x =  4.0306658472398675 y =  16.24634710608792 f= 0.0009410331310874199\n",
      "iter=  10 x =  4.030670911298055 y =  16.246346361465456 f= 0.0009408519970061899\n",
      "iter=  11 x =  4.03067207863791 y =  16.24633981576735 f= 0.0009408266299279954\n",
      "iter=  12 x =  4.030455179366049 y =  16.24432292192105 f= 0.0009335710734259546\n",
      "iter=  13 x =  4.0122806129474915 y =  16.096944312105617 f= 0.00036147108106582314\n",
      "iter=  14 x =  3.998789942906168 y =  15.990203248813819 f= 2.8509486934336762e-06\n",
      "iter=  15 x =  3.9987331509467756 y =  15.989859918513746 f= 1.6096592017318652e-06\n",
      "iter=  16 x =  3.9987324288364032 y =  15.989859292700459 f= 1.60704106215981e-06\n",
      "iter=  17 x =  3.9987323886169674 y =  15.989859339410856 f= 1.6070280560424958e-06\n",
      "iter=  18 x =  3.9987338106875336 y =  15.98987569927496 f= 1.6045389744182845e-06\n",
      "iter=  19 x =  3.9998481724111783 y =  15.998829034433419 f= 2.1342756522666655e-07\n",
      "iter=  20 x =  4.000026645073542 y =  16.000213470487655 f= 7.195197499051544e-10\n",
      "iter=  21 x =  4.000026685325492 y =  16.0002135146909 f= 7.122050347769995e-10\n",
      "iter=  22 x =  4.000026683111189 y =  16.00021351022491 f= 7.121875475889178e-10\n",
      "iter=  23 x =  4.000025918915644 y =  16.000206928695132 f= 6.897086303603234e-10\n",
      "iter=  24 x =  4.000001246226416 y =  16.00001034810395 f= 1.5863493344284757e-11\n",
      "iter=  25 x =  4.000000750076676 y =  16.00000600194824 f= 5.627930474260747e-13\n",
      "iter=  26 x =  4.000000750076676 y =  16.00000600194824 f= 5.627930474260747e-13\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rosenbrock(X,funcParams, doGradient):\n",
    "  \"\"\"\n",
    "  Compute the Rosenbrock function and optionally its gradient.\n",
    "  \n",
    "  The Rosenbrock function is defined as:\n",
    "      f(x, y) = (a - x)^2 + b * (y - x^2)^2\n",
    "  where 'a' and 'b' are parameters that control the shape of the function.\n",
    "\n",
    "  Parameters:\n",
    "      X (array-like): A 2-element array representing the point [x, y] in 2D space\n",
    "                      where the function and gradient are evaluated.\n",
    "      funcParams (array-like): A 2-element array containing the parameters [a, b]\n",
    "                                for the Rosenbrock function.\n",
    "      doGradient (bool): If True, the function also returns the gradient vector.\n",
    "                          If False, only the function value is returned.\n",
    "\n",
    "  Returns:\n",
    "      f (float): The value of the Rosenbrock function at the given point [x, y].\n",
    "      gradf (numpy.ndarray or None): The gradient vector [df/dx, df/dy] if doGradient is True.\n",
    "                                      If doGradient is False, returns None for the gradient.\n",
    "  \"\"\"\n",
    "  x = X[0]\n",
    "  y = X[1]\n",
    "  \n",
    "  a = funcParams[0]\n",
    "  b = funcParams[1]\n",
    "  \n",
    "  x2 = x*x\n",
    "  \n",
    "  t1 = a-x\n",
    "  t2 = y-x2\n",
    "  \n",
    "  f = t1**2 + b*t2**2\n",
    "  \n",
    "  if doGradient:\n",
    "    dfdx = - 2*t1 - 4*b*x*t2\n",
    "    dfdy = 2*b*t2\n",
    "  \n",
    "    gradf = np.array([dfdx,dfdy])\n",
    "  \n",
    "    return f, gradf\n",
    "  else:\n",
    "    return f, None\n",
    "\n",
    "def line_search(X, S, func, funcParams, maxIter, tol):\n",
    "    \"\"\"\n",
    "    Perform a golden section line search to find the optimal alpha that minimizes\n",
    "    the function F(X + alpha * S) given X and S\n",
    "    \n",
    "    Parameters:\n",
    "        X (array-like): Current point in the space.\n",
    "        S (array-like): Search direction.\n",
    "        tol (float): Tolerance for stopping criterion.\n",
    "        max_iter (int): Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        alpha_star (float): The optimal step size.\n",
    "    \"\"\"\n",
    "    # Golden ratio\n",
    "    phi = (1 + np.sqrt(5)) / 2\n",
    "    inv_phi = 1 / phi\n",
    "\n",
    "    # Initial interval [a, b]\n",
    "    a = 0\n",
    "    b = 1\n",
    "    # Evaluate points within the interval\n",
    "    alpha1 = b - inv_phi * (b - a)\n",
    "    alpha2 = a + inv_phi * (b - a)\n",
    "\n",
    "    # Compute objective function values at the points\n",
    "    f1 = func(X + alpha1 * S, funcParams, False)\n",
    "    f2 = func(X + alpha2 * S, funcParams, False)\n",
    "\n",
    "    # Iteratively narrow the search interval\n",
    "    for _ in range(maxIter):\n",
    "        if abs(b - a) < tol:\n",
    "            break\n",
    "\n",
    "        # Compare function values and update the interval\n",
    "        if f1 < f2:\n",
    "            b = alpha2\n",
    "            alpha2 = alpha1\n",
    "            f2 = f1\n",
    "            alpha1 = b - inv_phi * (b - a)\n",
    "            f1 = func(X + alpha1 * S, funcParams, False)\n",
    "        else:\n",
    "            a = alpha1\n",
    "            alpha1 = alpha2\n",
    "            f1 = f2\n",
    "            alpha2 = a + inv_phi * (b - a)\n",
    "            f2 = func(X + alpha2 * S, funcParams, False)\n",
    "\n",
    "    # Optimal step size is the midpoint of the final interval\n",
    "    alpha_star = (a + b) / 2\n",
    "    return alpha_star\n",
    "\n",
    "\n",
    "def fletcherReevesOpt(X0,func, funcParams, maxLineSearchIters, lineSearchTol, absTol, relTol):\n",
    "  \"\"\"\n",
    "  Perform the Fletcher-Reeves Conjugate Gradient optimization algorithm.\n",
    "\n",
    "  Parameters:\n",
    "      X0 (numpy.ndarray): The initial guess point for the optimization, e.g. a 2D vector [x, y].\n",
    "      func (callable): The objective function to be minimized. Should return \n",
    "                        the function value and its gradient at a given point.\n",
    "      funcParams (numpy.ndarray): Parameters required by the objective function.\n",
    "      maxLineSearchIters (int): The maximum number of iterations allowed for the line search.\n",
    "      lineSearchTol (float): Tolerance for the line search stopping criterion.\n",
    "      absTol (float): Absolute tolerance for convergence of the objective function.\n",
    "      relTol (float): Relative tolerance for convergence of the objective function.\n",
    "\n",
    "  Returns:\n",
    "      X (numpy.ndarray): The optimized point in the parameter space where the \n",
    "                          objective function reaches a minimum.\n",
    "      f (float): The value of the objective function at the optimized point.\n",
    "\n",
    "  \"\"\"\n",
    "  # Step 1: Initialize\n",
    "  X = X0\n",
    "  iters = 0\n",
    "  f, gradf = func(X, funcParams, True)\n",
    "  a = np.dot(gradf, gradf)\n",
    "  print(\"iter= \", iters, \"x = \", X[0], \"y = \", X[1], \"f=\", f)\n",
    "\n",
    "  \n",
    "  while True:\n",
    "\n",
    "    # If first time or if Fletcher-Reeves fails, then just select steepest decent\n",
    "    S = -gradf\n",
    "    \n",
    "    # Step 2: Line search to find optimal alpha\n",
    "    alpha_star = line_search(X, S, func, funcParams, maxLineSearchIters, lineSearchTol)\n",
    "\n",
    "    # Step 3: Check if alpha_star equals zero\n",
    "    if np.abs(alpha_star)<=1e-10:\n",
    "      print(\"Alpha is zero, algorithm exits.\")\n",
    "      break\n",
    "    \n",
    "    # Step 4: Update X\n",
    "    X = X + alpha_star * S\n",
    "    \n",
    "    # Step 5: compute function and gradient at new X\n",
    "    f, gradf = func(X, funcParams, True)\n",
    "\n",
    "    while True:\n",
    "      \n",
    "      # Compute Fletcher-Reeves conjugate direction update:\n",
    "      b = np.dot(gradf,gradf)\n",
    "      beta = b/a\n",
    "      S = -gradf + beta*S\n",
    "      a = b\n",
    "      \n",
    "      # if slope in the new S direction is greater than 0, break\n",
    "      if np.dot(S,gradf)>=0:\n",
    "        break\n",
    "      \n",
    "      # line search to find optimal alpha\n",
    "      alpha_star = line_search(X, S, func, funcParams, maxLineSearchIters, lineSearchTol)\n",
    "      \n",
    "      # compute new point and evaluate function\n",
    "      X_new = X + alpha_star * S\n",
    "      f_new, gradf_new = func(X_new, funcParams, True)\n",
    "\n",
    "      # check if convergence criteria are met\n",
    "      if np.abs(f_new - f)/np.abs(f)<= relTol or np.abs(f_new - f)<=absTol:\n",
    "        return X, f\n",
    "      else:\n",
    "        X = X_new\n",
    "        f = f_new\n",
    "        gradf = gradf_new\n",
    "        iters = iters + 1\n",
    "        print(\"iter= \", iters, \"x = \", X[0], \"y = \", X[1], \"f=\", f)\n",
    "    iters = iters + 1\n",
    "    \n",
    "  return X, f\n",
    "\n",
    "\n",
    "# Test Case\n",
    "def testCGWithRosenBrock():\n",
    "  X0 = np.array([-4.0,4.0])\n",
    "\n",
    "  a = 4\n",
    "  b = 100\n",
    "  rosenbrockFuncParams = np.array([a,b])\n",
    "  maxLineSearchIters = 100\n",
    "  lineSearchTol = 1e-5\n",
    "  relTol = 1e-12\n",
    "  absTol = 1e-16\n",
    "\n",
    "  fletcherReevesOpt(X0,rosenbrock, rosenbrockFuncParams, maxLineSearchIters, lineSearchTol, absTol, relTol)\n",
    "\n",
    "\n",
    "# Run test cases\n",
    "testCGWithRosenBrock()\n",
    "      \n",
    "\n",
    "      \n",
    "      \n",
    "\n",
    "  \n",
    "  \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
